{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# An implementation of bfs based on Grokking Algorithms chapter 6 and NOC \n",
    "# Bfs to find the shortest route to get to New York based on wikipedia articles\n",
    "# -------------\n",
    "\n",
    "from collections import deque\n",
    "import wikipedia\n",
    "wikipedia.set_lang(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define the graph and starting point\n",
    "graph = {}\n",
    "\n",
    "start_article = \"New York University\"# starting point\n",
    "search_term = 'Chile' # end point\n",
    "\n",
    "# create a Node class\n",
    "class Node:\n",
    "    def __init__(self, name, parent):\n",
    "        self.name = name\n",
    "        self.parent = parent\n",
    "        self.article = 'None'\n",
    "        self.content = 'None'\n",
    "        self.title = 'None'\n",
    "        self.links = 'None'\n",
    "            \n",
    "    def get_article(self):\n",
    "        try:\n",
    "            self.article = wikipedia.page(self.name) \n",
    "            self.content = self.article.content\n",
    "            self.title = self.article.title\n",
    "            self.links = self.article.links\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "# initialize the root node\n",
    "root_node = Node(start_article, None)\n",
    "root_node.get_article()\n",
    "graph[root_node.title] = []\n",
    "all_nodes = []\n",
    "\n",
    "for article in root_node.links:\n",
    "    graph[root_node.title].append(Node(article, root_node.title)) # add all of the first article links to the graph\n",
    "\n",
    "#print root_node.links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this is main loop\n",
    "def search(name):\n",
    "    print 'Searching how to get from ' + root_node.title + ' to ' + search_term\n",
    "    print '-------------------------------'\n",
    "    search_queue = deque() # create a new queue (double-ended queue)\n",
    "    search_queue += graph[root_node.title] # add all of the root node urls to the search queue\n",
    "    searched = [] # this array keeps track of which article we have already search for, so we dont search twice.\n",
    "    while search_queue: # while the queue isn't empty\n",
    "        article = search_queue.popleft() # grabs the first article off the queue\n",
    "        if not article.title in searched: # only search this article if we haven't already search for\n",
    "            article.get_article() # get the content from wikipedia\n",
    "            if search_term in article.content: # check if article contains the search_term we are looking for\n",
    "                all_nodes.append(article)\n",
    "                print 'Yes!, ' + search_term + ' is in ' +  article.title\n",
    "                current = article.title\n",
    "                path = []\n",
    "                while current != start_article:\n",
    "                    for article in all_nodes:\n",
    "                        if article.title == current:\n",
    "                            path.append(article.title)\n",
    "                            current = article.parent\n",
    "                path.append(start_article)\n",
    "                print ' -> '.join(path[::-1]) + ' -> ' + path[0] + ' has ' + search_term + ' in it!'\n",
    "                return True\n",
    "            else:\n",
    "                graph[article.title] = []\n",
    "                all_nodes.append(article)\n",
    "                for name in article.links: \n",
    "                    graph[article.title].append(Node(name, article.title)) # add all of the first article links to the graph\n",
    "                search_queue += graph[article.title]\n",
    "            searched.append(article.title) # mark this article as searched.\n",
    "    return False # if we reach here, the term was not find in the max of x iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching how to get from New York University to Chile\n",
      "-------------------------------\n",
      "Yes!, Chile is in Academy Awards\n",
      "New York University -> Academy Awards -> Academy Awards has Chile in it!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(start_article)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
